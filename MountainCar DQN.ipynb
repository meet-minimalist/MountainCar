{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will implement Deep Q Learning for Mountain Car environment.\n",
    "As we know that the Deep Q Learning takes states as an input to the deep neural network and computes Q values as a outputs.\n",
    "The squared error between expected Q value and computed Q value will be treated as an error for neural network. \n",
    "Based on this error the network parameters which are weights will be updated.\n",
    "\n",
    "## So lets begin.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"poster.jpg\">\n",
    "\n",
    "# Description\n",
    "Get an under powered car to the top of a hill (top = 0.5 position)\n",
    "\n",
    "# Environment\n",
    "## Observation\n",
    "Type: Box(2)\n",
    "\n",
    "| Num\t| Observation |\tMin | Max |\n",
    "| --- | --- | --- | --- |\n",
    "| 0 | position | -1.2 |\t0.6 |\n",
    "| 1 | velocity | -0.07 | 0.07 |\n",
    "\n",
    "\n",
    "## Actions\n",
    "Type: Discrete(3)\n",
    "\n",
    "| Num | Action |\n",
    "| --- | --- |\n",
    "| 0 | push left |\n",
    "| 1 | no push |\n",
    "| 2 | push right |\n",
    "\n",
    "\n",
    "## Reward\n",
    "-1 for each time step, until the goal position of 0.5 is reached. As with MountainCarContinuous v0, there is no penalty for climbing the left hill, which upon reached acts as a wall.\n",
    "\n",
    "## Starting State\n",
    "Random position from -0.6 to -0.4 with no velocity.\n",
    "\n",
    "## Episode Termination\n",
    "The episode ends when you reach 0.5 position, or if 200 iterations are reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space  Discrete(3)\n",
      "Observation space  Box(2,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space \", env.action_space)\n",
    "print(\"Observation space \", env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What will be stored inside a single memory\n",
    "# State, Action, Reward, Next Action\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "class memory_storage():\n",
    "    def __init__(self, mem_size):\n",
    "        self.memory = deque(maxlen = mem_size)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "    \n",
    "    def retrive_mem(self, batch_size):\n",
    "        idx = np.random.choice(range(len(self.memory)), size=batch_size)\n",
    "        return [self.memory[id] for id in idx]\n",
    "    \n",
    "    def generate_save_samples(self, no_episodes):\n",
    "        for i in range(no_episodes):\n",
    "            current_state = env.reset()\n",
    "            while True:\n",
    "                current_action = env.action_space.sample()\n",
    "                next_state, reward, done, _ = env.step(current_action)\n",
    "                if done == True:\n",
    "                    next_state = np.zeros(current_state.shape)\n",
    "                    self.memory.append([current_state, current_action, reward, next_state])\n",
    "                    break\n",
    "                else:\n",
    "                    self.memory.append([current_state, current_action, reward, next_state])\n",
    "                    current_state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we generate samples from no of multiple episodes\n",
    "myMemory = memory_storage(10000)\n",
    "myMemory.generate_save_samples(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Meet\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Our network will be 2 --> 10 --> 10 --> 3\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, ip, h1, h2, op, lr):\n",
    "        self.inputs_ = tf.placeholder(shape=[None, ip], dtype=tf.float32)\n",
    "        self.targets = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.actions_ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        one_hot_actions = tf.one_hot(tf.cast(self.actions_, tf.int32), op)\n",
    "        \n",
    "        self.w1 = tf.Variable(initial_value = tf.truncated_normal(shape=[ip, h1]))\n",
    "        self.b1 = tf.Variable(initial_value = tf.truncated_normal(shape=[h1]))\n",
    "        self.y1 = tf.nn.relu(tf.matmul(self.inputs_, self.w1) + self.b1)\n",
    "\n",
    "        self.w2 = tf.Variable(initial_value = tf.truncated_normal(shape=[h1, h2]))\n",
    "        self.b2 = tf.Variable(initial_value = tf.truncated_normal(shape=[h2]))\n",
    "        self.y2 = tf.nn.relu(tf.matmul(self.y1, self.w2) + self.b2)\n",
    "\n",
    "        self.w3 = tf.Variable(initial_value = tf.truncated_normal(shape=[h2, op]))\n",
    "        self.b3 = tf.Variable(initial_value = tf.truncated_normal(shape=[op]))\n",
    "        self.output_ = tf.matmul(self.y2, self.w3) + self.b3\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.output_, one_hot_actions), axis=1)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.squared_difference(self.Q, self.targets))\n",
    "        self.optimizer = tf.train.AdamOptimizer(lr).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_eps = 10000\n",
    "learning_rate = 0.001\n",
    "batch_retrive_size = 20\n",
    "gamma = 0.99\n",
    "decay_rate = 0.001\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "Episode reward:  -200.0\n",
      "-----------\n",
      "Episode: 1/10000 Average reward: -1.0 Loss: 0.5732564330101013\n",
      "-----------\n",
      "Episode reward:  -200.0\n",
      "-----------\n",
      "Episode: 2/10000 Average reward: -1.0 Loss: 0.23500461876392365\n",
      "-----------\n",
      "Episode reward:  -200.0\n",
      "-----------\n",
      "Episode: 3/10000 Average reward: -1.0 Loss: 0.0725632980465889\n",
      "-----------\n",
      "Episode reward:  -200.0\n",
      "-----------\n",
      "Episode: 4/10000 Average reward: -1.0 Loss: 0.013959852047264576\n",
      "-----------\n",
      "Episode reward:  -200.0\n",
      "-----------\n",
      "Episode: 5/10000 Average reward: -1.0 Loss: 0.10314229875802994\n",
      "-----------\n",
      "Episode reward:  -200.0\n",
      "-----------\n",
      "Episode: 6/10000 Average reward: -1.0 Loss: 0.044416218996047974\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2e9aa2412f69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mnext_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0mq_st\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyNeuralNetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mmyNeuralNetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_states\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mtarget_q\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_st\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1102\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m     \"\"\"\n\u001b[1;32m--> 492\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "myNeuralNetwork = NeuralNetwork(ip=env.observation_space.shape[0], h1=128, h2=128, op=env.action_space.n, lr=learning_rate)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "avg_reward = deque(maxlen=25)\n",
    "total_reward = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(total_eps):\n",
    "        episode_reward = 0\n",
    "        current_state = env.reset()\n",
    "        while True:\n",
    "            \"\"\"\n",
    "            take action based on either from network or on random basis\n",
    "            initially you need to take more random\n",
    "            but as the no of episodes are increasing the actions will be taken more with the help of network instead of random action\n",
    "            \"\"\"\n",
    "            \n",
    "            myVar = np.exp(-decay_rate*i)\n",
    "            if myVar > np.random.rand():\n",
    "                current_action = env.action_space.sample()\n",
    "            else:\n",
    "                op = sess.run(myNeuralNetwork.output_, feed_dict={myNeuralNetwork.inputs_: current_state.reshape((1, *current_state.shape))})\n",
    "                current_action = np.argmax(op)\n",
    "            \n",
    "            \n",
    "            next_state, reward, done, _ = env.step(current_action)\n",
    "            episode_reward += reward\n",
    "            if done == True:\n",
    "                next_state = np.zeros(current_state.shape)\n",
    "                myMemory.add([current_state, current_action, reward, next_state])\n",
    "                avg_reward.append(reward)\n",
    "                total_reward.append((i, episode_reward))\n",
    "                print(\"-----------\")\n",
    "                print(\"Episode reward: \", episode_reward)\n",
    "                print(\"-----------\")\n",
    "                break\n",
    "            else:\n",
    "                myMemory.add([current_state, current_action, reward, next_state])\n",
    "                avg_reward.append(reward)\n",
    "                current_state = next_state\n",
    "                \n",
    "                \n",
    "            # At the end of each time step we will update the network weights.\n",
    "            batch = myMemory.retrive_mem(batch_retrive_size)\n",
    "            batch_states = [a[0] for a in batch]\n",
    "            #print(np.array(batch_states).shape)\n",
    "            actions = [a[1] for a in batch]\n",
    "            rewards = [a[2] for a in batch]\n",
    "            next_s = [a[3] for a in batch]\n",
    "            \n",
    "            q_st = sess.run(myNeuralNetwork.output_, feed_dict={myNeuralNetwork.inputs_: batch_states})\n",
    "            \n",
    "            target_q = rewards + gamma*np.max(q_st, axis=1)\n",
    "            #print(np.array(target_q).shape)\n",
    "            \n",
    "            loss, _ = sess.run([myNeuralNetwork.loss, myNeuralNetwork.optimizer], \n",
    "                               feed_dict={myNeuralNetwork.inputs_: batch_states, myNeuralNetwork.targets: target_q, myNeuralNetwork.actions_: actions})\n",
    "    \n",
    "        print(\"Episode: {}/{}\".format(i+1, total_eps), \n",
    "              \"Average reward: {}\".format(np.mean(avg_reward)),\n",
    "              \"Loss: {}\".format(loss))\n",
    "        \n",
    "    saver.save(sess, \"checkpoints/mountaincar_model.ckpt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state = env.reset()\n",
    "current_state.reshape((1, current_state.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eps = 10\n",
    "test_max_step = 400\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(\"checkpoints\"))\n",
    "    t_reward = []\n",
    "    for ep in range(test_eps):\n",
    "        t = 0\n",
    "        eps_reward = 0\n",
    "        current_state = env.reset()\n",
    "        while t < test_max_step:\n",
    "            env.render()\n",
    "            \n",
    "            action = sess.run(myNeuralNetwork.output_, feed_dict={myNeuralNetwork.inputs_: current_state.reshape(1, *current_state.shape)})\n",
    "            \n",
    "            action = np.argmax(action)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            eps_reward += reward\n",
    "            \n",
    "            if done == True:\n",
    "                t_reward.append((ep, eps_reward))\n",
    "                break\n",
    "            else:\n",
    "                current_state = next_state\n",
    "                t += 1\n",
    "    \n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
